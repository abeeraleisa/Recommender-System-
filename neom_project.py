# -*- coding: utf-8 -*-
"""Neom_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b_SGRIB_EOKpFMy3xvmGgwrUcD8wP7iX
"""

from google.colab import drive
drive.mount('/content/drive')

"""# NEOM PROJECT

## Preprocessing
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# import the data (chunksize returns jsonReader for iteration)
businesses = pd.read_json("/content/drive/My Drive/Neom comp/yelp_academic_dataset_business.json", lines=True, orient='columns', chunksize=1000000)
reviews = pd.read_json("/content/drive/My Drive/Neom comp/yelp_academic_dataset_review.json", lines=True, orient='columns', chunksize=1000000)

# read the data 
for business in businesses:
    subset_business = business
    break
    
for review in reviews:
    subset_review = review
    break

subset_review.head()

starsyelp_data = pd.merge(subset_business , subset_review)

yelp_data =  yelp_data.drop(['postal_code', 'state', 'is_open', 'useful', 'funny', 'cool', 'text', 'date' ,'hours', 'address'], axis=1)

yelp_data['user_id_code'] = yelp_data.business_id.astype('category').cat.codes
yelp_data['business_id_code'] = yelp_data.user_id.astype('category').cat.codes
yelp_data['review_id_code'] = yelp_data.review_id.astype('category').cat.codes

yelp_data.rename(columns={'user_id_code':'userId', 'business_id_code':'businessId',
                  'review_id_code':'reviewId'}, inplace=True)

yelp_data =  yelp_data.drop(['business_id', 'review_id','user_id'], axis=1)

yelp_data.head()

# average rating for each business
yelp_data.groupby('businessId')['stars'].mean()

# number of reviews per business
yelp_data.groupby('businessId')['userId'].count().sort_values(ascending=False).head(10)

yelp_data['ratings_per_business'] = yelp_data.groupby('businessId')['userId'].transform('count')

total_users = yelp_data['userId'].nunique()
total_users

ratings = pd.DataFrame(yelp_data.groupby('businessId')['stars'].mean())

ratings['rating_counts'] = pd.DataFrame(yelp_data.groupby('businessId')['stars'].count())
max_rating_counts = ratings['rating_counts'].max()
max_rating_counts

ratings.head(20)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('dark')
# %matplotlib inline

# add a new column 'total_visitors', and generate random values
for i, row in ratings.iterrows():
    ratings.loc[i, 'total_visitors'] = np.random.randint(row['rating_counts']*3, row['rating_counts']*40)

ratings[:20].plot(y=['rating_counts','total_visitors'], figsize=(10,5))

ratings['rating_percentage'] = ratings['rating_counts'] / ratings['total_visitors']
ratings.head(10)

"""## NEURAL NETWORKS"""

#Split the data into train and test sets.
from sklearn.model_selection import train_test_split

X = yelp_data[['userId', 'businessId']].values
y = yelp_data['stars'].values

X_train_keras, X_test_keras, y_train_keras, y_test_keras = train_test_split(X, y, test_size=0.2, random_state=42)
X_train_keras.shape, X_test_keras.shape, y_train_keras.shape, y_test_keras.shape

n_factors = 10

X_train_array = [X_train_keras[:, 0], X_train_keras[:, 1]]
X_test_array = [X_test_keras[:, 0], X_test_keras[:, 1]]

from keras.layers import Add, Activation, Lambda
from keras.models import Model
from keras.layers import Input, Reshape, Dot
from keras.layers.embeddings import Embedding
from keras.optimizers import Adam
from keras.regularizers import l2

class EmbeddingLayer:
    def __init__(self, n_items, n_factors):
        self.n_items = n_items
        self.n_factors = n_factors
    
    def __call__(self, x):
        x = Embedding(self.n_items, self.n_factors, embeddings_initializer='he_normal', embeddings_regularizer=l2(1e-6))(x)
        x = Reshape((self.n_factors,))(x)
        
        return x
    
def Recommender(n_users, n_rests, n_factors, min_rating, max_rating):
    user = Input(shape=(1,))
    u = EmbeddingLayer(n_users, n_factors)(user)
    ub = EmbeddingLayer(n_users, 1)(user)
    
    restaurant = Input(shape=(1,))
    m = EmbeddingLayer(n_rests, n_factors)(restaurant)
    mb = EmbeddingLayer(n_rests, 1)(restaurant)   
    
    x = Dot(axes=1)([u, m])
    x = Add()([x, ub, mb])
    x = Activation('sigmoid')(x)
    x = Lambda(lambda x: x * (max_rating - min_rating) + min_rating)(x)  
    
    model = Model(inputs=[user, restaurant], outputs=x)
    opt = Adam(lr=0.001)
    model.compile(loss='mean_squared_error', optimizer=opt)  
    
    return model

n_users = yelp_data['userId'].nunique()
n_business = yelp_data['businessId'].nunique()
min_rating = min(yelp_data['stars'])
max_rating = max(yelp_data['stars'])

keras_model = Recommender(n_users, n_business , n_factors, min_rating, max_rating)
keras_model.summary()

# Let’s go ahead and train this for a few epochs and see what we get.
keras_model.fit(x=X_train_array, y=y_train_keras, batch_size=64,\
                          epochs=50, verbose=1, validation_data=(X_test_array, y_test_keras))

predictions = keras_model.predict(X_test_array)

#By creating the following table, we are able to see the model performance by comparing the actual stars and predictions.

# create the df_test table with prediction results
df_test = pd.DataFrame(X_test_keras[:,0])
df_test.rename(columns={0: "user"}, inplace=True)
df_test['business'] = X_test_keras[:,1]
df_test['stars'] = y_test_keras
df_test["predictions"] = predictions
df_test.head()

"""### Evaluation"""

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

y_test_keras.shape

predictions.shape

y_test_keras.reshape((29190, ))

predictions.reshape((29190, ))

mae = mean_absolute_error(y_test_keras,  predictions)

mae

mse= mean_squared_error(y_test_keras ,  predictions)

mse

def rmse(predictions, targets):
    return np.sqrt(((predictions - targets) ** 2).mean())

rmse_val = rmse(y_test_keras ,  predictions)

rmse_val

"""## Pearson Correlation Coefficient"""

yelp_data.head(100)

businesses = pd.read_csv('/content/business.csv')
review = pd.read_csv('/content/user.csv')

businesses.head(2)

review=review.drop(['Unnamed: 3'], axis=1)

review.head(2)

userInput = [
{'businessName':'Algerian Coffee Stores', 'rating':1},
{'businessName':'Karam Beirut', 'rating':1},
{'businessName':'I stan', 'rating':1},
{'businessName':'Neom beach', 'rating':0.5},
{'businessName':'Neom exhbit for Saudi culture', 'rating':0}
]
inputData = pd.DataFrame(userInput)
inputData

#Filtering out the movies by title
inputId = businesses[businesses['businessName'].isin(inputData ['businessName'].tolist())]
#Then merging it so we can get the movieId. It’s implicitly merging it by title.
inputData  = pd.merge(inputId, inputData )
#Dropping information we won’t use from the input dataframe
#inputMovies = inputMovies.drop(‘year’, 1)
#Final input dataframe
#If a movie you added in above isn’t here, then it might not be in the original
#dataframe or it might spelled differently, please check capitalisation.
inputData

inputId

#Filtering out users that have watched movies that the input has watched and storing it
userSubset = review[review['businessId'].isin(inputData ['businessId'].tolist())]
userSubset.head()

#Groupby creates several sub dataframes where they all have the same value in the column specified as the parameter
userSubsetGroup = userSubset.groupby(['userId'])

#Sorting it so users with movie most in common with the input will have priority
userSubsetGroup = sorted(userSubsetGroup, key=lambda x: len(x[1]), reverse=True)

userSubsetGroup = userSubsetGroup[0:100]

from math import sqrt

#Store the Pearson Correlation in a dictionary, where the key is the user Id and the value is the coefficient
pearsonCorrelationDict = {}
#For every user group in our subset
for name, group in userSubsetGroup:
    #Let's start by sorting the input and current user group so the values aren't mixed up later on
    group = group.sort_values(by='businessId')
    inputData = inputData.sort_values(by='businessId')
    #Get the N for the formula
    nRatings = len(group)
    #Get the review scores for the movies that they both have in common
    temp_df = inputData[inputData['businessId'].isin(group['businessId'].tolist())]
    #And then store them in a temporary buffer variable in a list format to facilitate future calculations
    tempRatingList = temp_df['rating'].tolist()
    #Let's also put the current user group reviews in a list format
    tempGroupList = group['rating'].tolist()
    #Now let's calculate the pearson correlation between two users, so called, x and y
    Sxx = sum([i**2 for i in tempRatingList]) - pow(sum(tempRatingList),2)/float(nRatings)
    Syy = sum([i**2 for i in tempGroupList]) - pow(sum(tempGroupList),2)/float(nRatings)
    Sxy = sum( i*j for i, j in zip(tempRatingList, tempGroupList)) - sum(tempRatingList)*sum(tempGroupList)/float(nRatings)
    
    #If the denominator is different than zero, then divide, else, 0 correlation.
    if Sxx != 0 and Syy != 0:
        pearsonCorrelationDict[name] = Sxy/sqrt(Sxx*Syy)
    else:
        pearsonCorrelationDict[name] = 0

pearsonCorrelationDict.items()

pearsonDF = pd.DataFrame.from_dict(pearsonCorrelationDict, orient='index')
pearsonDF.columns = ['similarityIndex']
pearsonDF['userId'] = pearsonDF.index
pearsonDF.index = range(len(pearsonDF))
pearsonDF.head()

topUsers=pearsonDF.sort_values(by='similarityIndex', ascending=False)[0:50]
topUsers

topUsersRating=topUsers.merge(review, left_on='userId', right_on='userId', how='inner')
topUsersRating.head()

#Multiplies the similarity by the user's ratings
topUsersRating['weightedRating'] = topUsersRating['similarityIndex']*topUsersRating['rating']
topUsersRating.head(20)

#Applies a sum to the topUsers after grouping it up by userId
tempTopUsersRating = topUsersRating.groupby('businessId').sum()[['similarityIndex','weightedRating']]
tempTopUsersRating.columns = ['sum_similarityIndex','sum_weightedRating']
tempTopUsersRating.head()

#Creates an empty dataframe
recommendation_df = pd.DataFrame()
#Now we take the weighted average
recommendation_df['weighted average recommendation score'] = tempTopUsersRating['sum_weightedRating']/tempTopUsersRating['sum_similarityIndex']
recommendation_df['businessId'] = tempTopUsersRating.index
recommendation_df.head()

recommendation_df = recommendation_df.sort_values(by='weighted average recommendation score', ascending=False)
recommendation_df.head(10)

recommendation_df=recommendation_df.replace([np.inf, -np.inf], np.nan)

recommendation_df=recommendation_df.dropna()

recommendation_df

recommendations = businesses[businesses['businessId'].isin(recommendation_df ['businessId'].tolist())]

recommendations.head(10)

